\section{Introduction}
\label{section:introduction}

The world wide web has evolved to become a common source of information and entertainment for people from all around the globe. The number of active websites exceeds 100 million by far. People commonly access these websites through the internet using a webbrowser on their computer or hand-held devices and consume the content in graphical or auditory format.

Some website attract more visitors than others. As of January 2018, the Alexa page ranking\footnote{The top 500 sites on the web: \url{https://www.alexa.com/topsites}} lists \url{google.com}, \url{youtube.com}, \url{facebook.com}, \url{baidu.com}, and \url{wikipedia.org} as the top five websites. Ordering websites by their popularity can be done in different ways: One might think of the total number of links pointing to a website, the visitor count, or visitor count combined with the average time people spend.

Newly published websites will initially be assigned a low rank, until their popularity grows and statistics start reflecting that. An experienced user, however, might be able to have a rough feeling for whether a website they see has potential to become popular or not, solely based on its content and look. To the best of our knowledge, there is a shortcoming in tools, which estimate the expected popularity of a website, thereby replicating the behavior of a human visitor.

In this work we model a pagerank estimator using methods of computer vision. More specifically, we train a graph network with convolutional building blocks to \textit{look} at screenshots taken from web page of a domain and try to predict the expected pagerank.

The world wide web has been designed with hyperlinks. They allow users to navigate between web pages. A website typically consists of several web pages, which (often, but not necessarily) point to each other with hyperlinks. Such a website can be interpreted as a graph, where nodes correspond to web pages and edges to hyperlinks connecting two of them. We try to exploit the information contained in the graph structure by applying graph networks to the ranking problem.

The advantages over statistical approaches (link counting, visitor counters with toolbars, etc.) are the following: Once trained, our model can generalize to new, unseen websites, without need to spend time crawling the web. It is purely vision-based and has therefore access to (most of) the information that website visitors consume. On the other hand, the ground truth used to train our model is the result of the aforementioned statistical analyses. Consequently, its  predictions cannot be better on known website.

Traversing the web page graph of the internet or a single website is referred to as crawling. In order to create a screenshot dataset that contains the meta information we want, we have developed a webcrawler. It has visited 100,000 web pages and took screenshots of them. The crawler is described in this document as well.

Our main contributions are:
\begin{itemize}
\item Creation and release of a pagerank dataset with two versions: (v1) A single screenshot for each of the top 100k websites and (v2) 100k graphs, where each graph represents one of the websites with screenshots and meta data.
\item Application of graph networks to the problem of pagerank prediction.
\item Answering of the question to what degree purely vision-based information from websites is sufficient to estimate their rank.
\end{itemize}

This work is a student research project carried out by two students of Applied Computer Science from Coorporative State University Baden-Wuerttemberg, Karlsruhe.

The remainder of this document is structured as follows: \todo{Write one sentence per section.}

The mathematical notation used throughout this document follows the one defined in the chapter \textit{Notation} of the Deep Learning book by \cite{goodfellow:dlbook}.
