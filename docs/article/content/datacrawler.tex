\section{Datacrawler}
\label{Datacrawler}
The main goal of the datacrawler is to deliver the required datasets as specified in \ref{datasets}. The input to the datacrawler is simply a domain, which is provided by the ground truth. Depending on the configuration the output of the datacrawler can vary from a single screenshot (\ref{DatasetVersion1}) to a complete graph of the given domain (\ref{DatasetVersion2}).


The following section (\ref{datacrawler_requirements}) will profoundly discuss the requirements in functionality for the datacrawler, which will serve as a basis for discussion for the decision between the frameworks \texttt{Puppeeter} and \texttt{Chromium Embedded Framework} (\ref{datacrawler_framework_language}). Afterwards we will give an overview of the architecture (\ref{datacrawler_architecture}) and highlight the idea of \textit{DataModules} in the crawler (\ref{datacrawler_datamodulesystem}). Following that, we will discuss profoundly all developed \textit{DataModules} with their role in the datacrawler (\ref{datacrawler_screenshot_datamodule}, \ref{datacrawler_mobilescreenshot_datamodule}, \ref{datacrawler_url_datamodule}). We will wrap up the internals of the datacrawler by taking a look into the workflow of the datacrawler (\ref{datacrawler_workflow}).

In the last section (\ref{datacrawler_scale}) we will discuss how we successfully designed a sophisticated system to scale-out the datacrawler reducing the total dataset creation time on Google Cloud Platform.  

\subsection{Requirements}
\label{datacrawler_requirements}
The requirements in functionality for the datacrawler arise from the dataset specifications (\ref{DatasetVersion1}, \ref{DatasetVersion2}). The next section will derive the requirements from the dataset specifications and discuss them profoundly.

\subsubsection{Browser Emulation}
\label{browser_emulation}
One of the major requirements for the datacrawler is to find a convenient way of emulating a web browser. According to the dataset specifications, for every given website a screenshot $\tensorsym{I}$ has to be taken. In addition to that, every screenshot must represent the website as the user would see in a common browser. The latter and other attributes such as the loading time $l$ of a website like in a common browser make the emulation of a browser an inevitable requirement. 

\subsubsection{Information Accessibility}
\label{information_accessibility}
Accessing \textit{low-level} information such as the \textit{HTTP-Request} to change the \textit{user-agent} to generate a mobile screenshot $\tensorsym{M}$ (\ref{datacrawler_mobilescreenshot_datamodule}) or the \textit{Document Object Model (DOM)} (\ref{datacrawler_url_datamodule}) to generate edges $a$ in the graph according to dataset version 2 specification (\ref{DatasetVersion2}) is inevitable. The datacrawler has to be able to manipulate internal data structures and even able to inject own \textit{JavaScript-code} (\ref{datacrawler_screenshot_datamodule}) on the website.

\subsubsection{Modularity}
\label{modularity}
The datacrawler has to be as modular for allowing us to extend the datacrawler easily and decide which attribute should be calculated. This requirement in flexibility is raised from the fact that our dataset specifications might change over the time or new ones might be added.

This flexibility led to a sophisticated architecture such as the \text{DataModule-System} (\ref{datacrawler_datamodulesystem}), which makes it possible to pass the dataset specification directly into the datacrawler.

\subsubsection{Scalability}
\label{scalability}
Both dataset versions require the analysis of at least 100,000 websites. Therefore, datacrawler has to be horizontally scalable to allow the analysis of multiple domains at once, and efficient in terms analysis time and start-up time. An inefficient and not scalable datacrawler would lead to multiple days of required analysis time and reflect also in high infrastructure costs.

\subsection{Framework}
\label{datacrawler_framework_language}
The previous section has shown some of the intricate requirements for the datacrawler. To answer those requirements in the given time frame, we investigated in frameworks, which would do most of the heavy-lifting for us such as networking, I/O or rendering of the website.

This section will introduce two frameworks \texttt{Puppeteer} and \texttt{Chromium Embedded Framework} (\texttt{CEF}), compare both and discuss our decision for \texttt{CEF}. Both frameworks are providing an API to instrument the well-known browser \texttt{Chrome}.

\subsubsection{Puppeteer}
\texttt{Puppeteer} is a open-source library for the JavaScript runtime \texttt{Node.js}, which provides a high-level API to control instances of the browsers \texttt{Chrome} or \texttt{Chromium}. One of the main goals of \texttt{Puppeteer} is to grow adoption in automated browser testing \cite{PuppeteerFAQ}.  For this purpose, it wraps the \texttt{Chrome DevTools Protocol} in JavaScript, which allows web developers to instrument, inspect and debug instances of \texttt{Chrome}. Technically, the protocol is exposed at the port for debugging by \texttt{Chrome}.

The protocol offers a varies number of functionalities, which are grouped into domains \cite{DevToolsProtocol}. Some of the interesting domains for our use-case are:
\begin{description}
	\item[Page]: API to load and take a screenshot of the given website.
	\item[DOM]: API to read and manipulate the DOM of the given website.
	\item[Network]: API to intercept network requests, track downloaded data and network issues.
	\item[Emulation]: API to emulate different geolocation, network bandwith and mobile devices.
\end{description}

During start up \texttt{Puppeteer} starts an instance of \texttt{Chrome}, attaches to the instances using the port for debugging and afterwards custom code of the user will be executed against the instance. The instance will be started in \texttt{headless}-mode meaning that no UI will be visible.

\texttt{Puppeteer} fulfills all requirements (\ref{datacrawler_requirements}) raised from the dataset specifications:
The requirement of \textbf{Browser Emulation} (\ref{browser_emulation}), since it is using an instance of \texttt{Chrome} to render websites. The requirement of \textbf{Information Accessibility} (\ref{information_accessibility}), due to functionalities in the domains DOM and Network. Furthermore \textbf{Modularity} (\ref{modularity}) is given per se by using JavaScript as the programming language, which makes the use of polymorphism possible as used in DataModule-System (\ref{datacrawler_datamodulesystem}). The usage of \texttt{Chrome} ensures efficiency in terms of loading, rendering websites and retrieval of website information. Moreover, the combination of \texttt{Chrome} and \texttt{Puppeteer} can be scaled-out easily fulfilling the last requirement \textbf{Scalability} (\ref{scalability}) by using our system described in \ref{datacrawler_scale}.

\subsubsection{Chromium Embedded Framework}
\texttt{Chromium Embedded Framework (CEF)} is an open-source framework for embedding the \texttt{Chromium} browser into other applications. The main goal of \texttt{CEF} is to enable developers in adding web browsing functionality such as using HTML, CSS and JavaScript to create application UI. Well-known applications such as \texttt{Adobe Acrobat}, \texttt{Spotify Desktop} and \texttt{MATLAB} are using \texttt{CEF}.

\texttt{CEF} was designed ground-up with performance and ease-use in mind. The framework exposes \texttt{C++} interfaces with default implementation for all features requiring little or no integration work. Furthermore, the community added wrappers for the base implementation to support a wide-range of operating systems and programming languages.

\texttt{CEF} has in total three versions, whereas \texttt{CEF 2} was abandoned due to the appearance of the \texttt{Chromium Content API} which will be discussed later. \texttt{CEF 1} is a single process implementation and based on the old \texttt{Chromium WebKit API}, due to deprecation of the \texttt{Chromium WebKit API} it is no longer supported and developed. \texttt{CEF 3} is a multi process implementation and based on the current \texttt{Chromium Content API}.

As the \texttt{Chromium} code base has grown, it was inevitable to avoid opaque dependencies and features in the wrong places. Therefore the \textit{content module} was introduced, which contains the core functionality of \texttt{Chromium}. The \texttt{Chromium Content API} wraps the \texttt{content module} and offers isolation for developers from the core functionalities \cite{ChromiumContentAPI}.
\texttt{CEF 3} insulates the user from the underlying complexity of \texttt{Chromium} by using the \texttt{Chromium Content API} and offering production-quality stable APIs \cite{CEFGeneralUsage}. The offered and consumed APIs by the datacrawler will be profoundly discussed in \textbf{Datacrawler Architecture} (\ref{datacrawler_architecture}).

Through the use of \texttt{Chromium Content API}, \texttt{CEF 3} provides a close integration between the browser and the host application including support for custom JavaScript objects and JavaScript extensions. Moreover, the host application is able to control resource loading, intercept the network, navigation and many more, while taking advantage of the same performance and technologies available in the \texttt{Google Chrome Web browser} \cite{CEFGeneralUsage}.

\texttt{CEF 3} clearly fulfills the requirement of \textbf{Browser Emulation} (\ref{browser_emulation}) and \textbf{Information Accessibility} (\ref{information_accessibility}). Moreover, \textbf{Modularity} (\ref{modularity}) and \textbf{Scalability} (\ref{scalability}) is also given by using \texttt{C++} and the core functionalities of \texttt{Chromium}.

\subsubsection{Framework Decision}

\subsection{Datacrawler Architecture}
\label{datacrawler_architecture}

\subsubsection{DataModule-System}
\label{datacrawler_datamodulesystem}

\subsubsection{Screenshot-Datamodule}
\label{datacrawler_screenshot_datamodule}

\subsubsection{MobileScreenshot-Datamodule}
\label{datacrawler_mobilescreenshot_datamodule}

\subsubsection{URL-Datamodule}
\label{datacrawler_url_datamodule}

\improve{[IMPROVE] Add documentation for new datamodules}

\subsubsection{Datacrawler Workflow}
\label{datacrawler_workflow}

\subsection{Scaling the Datacrawler}
\label{datacrawler_scale}

\subsubsection{Requirements}
\label{datacrawler_scale_requirements}

\subsubsection{The Scale Architecture}
\label{datacrawler_scale_architecture}


