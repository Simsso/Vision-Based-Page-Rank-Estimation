\section{Results}

\subsection{Human Evaluation}
Most of machine learning algorithms are developed to imitate and automate human like performance on a hard problem such as the classification of animals in images. Human performance is often seen as the baseline for machine learning algorithms to outperform. As part of our research we evaluate the performance of humans on vision-based pagerank estimation of websites and compare it with our current novel approach the usage of deep graph networks. 

Therefore, the following sections will introduce our experimental setup and the results of the human evaluation. Afterwards we will compare the results with our current research and discuss the criteria on which the test persons based their decisions.

\subsubsection{Experimental Setup} 
The human performance on page rank estimation is measured solely based on the vision of websites. In our experimental setup each test person has to select the website with the higher rank from a pair of different and randomly chosen websites from the dataset version 2. In addition to reduce the distortion in our results caused by additional knowledge other than vision of the website, we advised the test persons to skip website they know. The selection is supported by the domain names and all mobile and desktop screenshots of the given websites from the dataset version 2. For each test person we calculate the accuracy from the number of correct and total classified websites, which is shown during the evaluation to each test person. In general there is no time limit for the decision making and the test is repeated for $n$ pairs. The whole evaluation is taken place in a web application solely developed for the human evaluation.

\subsubsection{Results}
We evaluated in total of 11 test persons regarding their performance on estimating pagerank of websites, who rated in total $1900$ pairs of websites. The average performance of humans is at $57.8$\% and with accuracies ranging from $51.67$\% (lowest) to $64$\% (highest), while our best performing deep graph network has achieved the accuracy of $63$\% as table \ref{table_human_eval_results} shows.

We ensure using a $z$-test that our test persons are performing better than random guessing ($50$\%) and the accuracies are not compromised by noise with a significance level of $p << 1$\%. In addition, we ensure using a \textit{Welch's} $t$-student test with $p << 1$\% that our deep graph network outperforms humans.

\begin{table}[]
	\center
	\begin{tabular}{c||l}
		Test Person \#  & ($n$, accuracy)\\ \hline \hline
		1 &  $(100, 60\%)$ \\ \hline
		2 &  $(100, 64\%)$\\ \hline
		3 &  $(100, 58\%)$\\ \hline
		4 &  $(100, 57\%)$, $(100, 62\%)$, $(500, 56.8\%)$\\ \hline
		5 &  $(100, 56\%)$\\ \hline
		6 &  $(100, 60\%)$\\ \hline
		7 &  $(300, 51.67\%)$\\ \hline
		8 &  $(200, 57\%)$\\ \hline
		9 &  $(200, 54\%)$\\
	\end{tabular}
	\caption[Results of Human Performance on Page Rank Estimation]{The table shows the results in our experiments on the performance of Humans on page rank estimation. While the lowest accuracy was achieved by test person 7 with $51.67$\%, the highest accuracy was $64$\% by test person 1.}
\label{table_human_eval_results}
\end{table}

\subsubsection{Criteria}
By the end of the experiment, each test person was asked about their criteria used during the evaluation on which they based their decisions. We summarized positive and negative criteria on which the test persons have based their decision:

\begin{itemize}
	\item Positive: \begin{enumerate}
	 \item Responsive and modern looking websites
	 \item Official, game, news and personal websites of seemingly famous people
	 \item High-content density
	 \item Matching colors
	 \item Professional looking
	 \item Article-like
	\end{enumerate}
	\item Negative: \begin{enumerate}
	\item Unresponsive and old looking websites
	\item Low-content density
	\item Personal sites of seemingly unpopular persons
	\item Websites with placeholders such as domain parking or errors
	\end{enumerate}
\end{itemize}

The listed criteria clearly shows that the test persons did not estimate the page rank solely based on aspects in vision, rather also on the content and errors they saw. The test persons emphasized that they ranked matching colors, overall responsive and professional looking websites higher than old and unresponsive looking websites. In addition, another major factor was the type of the given website. Thus, news, games and official websites were clearly ranked more often higher than their counterparts.

\subsubsection{Conclusion}
The human evaluation has shown us that we were able to outperform human performance in page rank estimation of website, despite the fact that estimation of page rank being a very difficult problem for both humans and machines. We have to emphasize that the test persons primarily decided on their prior-experience and perception of good and bad websites, which is ultimately distorted by the common perception and understanding of websites in 2019.