\section{Results}

We report results on the pairwise page rank estimation task and compare several variants of our models with each other. As a reference point we consider the human score on the task as well.

In the \textbf{pairwise page rank estimation task}, the model (or human) is asked to make a guess about the relative ordering of two samples $G^{(i)}$ and $G^{(j)}$ drawn from a subset of the dataset. There are three possible answers: (1) both samples have the same rank, (2) sample $G^{(i)}$ has a higher rank than $G^{(j)}$, and (3) sample $G^{(j)}$ has a lower rank than $G^{(i)}$. A guess is either right or wrong, depending on the ground truth, i.e. $r^{(i)}$ and $r^{(j)}$.

Given multiple answers to pairwise ranking questions, the accuracy can be computed as described in Equation \ref{eq:acc}. This accuracy is the main evaluation metric we report in this section and the comparison of models is also based on it.

Unless indicated otherwise, we train with the following configuration:
\begin{itemize}
    \item Dataset version 2 (Section \ref{DatasetVersion2}), split into train, validation, and test split. We train on 60\% of the dataset, tune hyperparameters on 20\% (validation), and use the remaining 20\% for final performance reporting. The samples are assigned to the dataset splits across all ranks. E.g. we do not use a specific range of ranks as our test set. See Listing \ref{lst:getthreefold} for the dataset splitting algorithm. All accuracy scores are computed on unseen test samples; exceptions are specified.
    \item Parameters are updated by the Adam optimizer \cite{kingma2014:adam}, parameterized with learning rate $\eta=10^{-4}$, $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$. We do not use weight decay as overfitting was not an issue. The learning rate is kept constant throughout the training.
    \item We employ dropout \cite{srivastava2014:dropout} at a rate of $p_\text{drop}=0.1$.
    \item Screenshots, both desktop and mobile, are normalized across all three channels (RGB) to have a mean of $0.5$ and standard deviation of $0.5$. No other data augmentation is used.
    \item The weighting function's base is set to $b=2$.
    \item Training were run on GPU; specifically a Nvidia GTX 970 with 4 GB of RAM.
\end{itemize}

When introducing our model in Section \ref{sec:method} we have already enumerated three different ways of training it (end-to-end training [e2e], pre-training without fine-tuning [woft], and pre-training with fine-tuning [wft]). Due to GPU memory constraints, the training modes [e2e] and [wft] allow for a maximum batch size of two, which we is seemingly harmful to proper gradient approximation. Besides that, both training modes require gradient propagation through the GN part of the model which is relatively slow. Except for the comparison of the three approaches, which can be found in Table \ref{tab:trainmethodcomparison} we have used [woft] for all experiments.

For [woft] we attach a linear layer to the screenshot feature extractor and train it to output a scalar that ranks webpages. That way the model has only access to individual images without any context, i.e. screenshots of other pages, mobile/desktop in parallel, or graph edges. The loss function from Equation \ref{eq:loss} is slightly modified to account for the variable number of screenshots: We weigh the loss with the inverse number of screenshots so the rank prediction of a sample with a single webpage has the contribution to the loss as a sample with e.g. eight screenshots combined. After convergence of the feature extractor we compute all feature vectors for the entire dataset and use them as input to the GN part of our model. That way we reduce the computational overhead significantly by bypassing the CNN in the GN training. This gave us headroom for a greater number of experiments with GN variants.

\begin{table}
    \centering
    \begin{tabular}{lrrr}
        \textbf{Training} & \textbf{Accuracy} & \textbf{Wall-clock time}\\\hline
        \textbf{[e2e]} & $xx.x\%$ & xx h\\
        \textbf{[wft]} & $xx.x\%$ & xx h\\
        \textbf{[woft]} & $xx.x\%$ & xx h\\
        \textbf{[woft] + GN} & $xx.x\%$ & xx h\\
    \end{tabular}
    \caption[Model training method comparison]{Model training method comparison: end-to-end training [e2e], pre-training without fine-tuning [woft], pre-training with fine-tuning [wft]. The accuracy is reported on the held-out test set; wall-clock time is the time until convergence.}
    \label{tab:trainmethodcomparison}
\end{table}

Our loss term contains a weighting function (Equation \ref{eq:weightingfactor}) $w_b$. We evaluate its importance by fine tuning a graph network attached to a pre-trained feature extractor. For computation reasons, the feature extractor is pre-trained with $w_{10}$, so $b$ is only varied for the GN training. The results can be found in Table \ref{tab:weightingbase}. The model trained with $b=xxx$ achieves the best accuracy on the test set.

\begin{table}
    \centering
    \begin{tabular}{lrrr}
        \textbf{Weighting $b$} & \textbf{Accuracy}\\\hline
        $w_b=1$ & $xx.x\%$\\
        $b=1$ & $xx.x\%$\\
        $b=2$ & $xx.x\%$\\
        $b=10$ & $xx.x\%$\\
        $b=100$ & $xx.x\%$\\
    \end{tabular}
    \caption[Effect of the sample weighting function]{Effect of the sample weighting function}
    \label{tab:weightingbase}
\end{table}

How to deal with dataset v1 here? Perhaps: Served as an experimentation platform and we did not max it out. Our best accuracy was xxx but that can likely be improved with tuning.

Graph structure usage: recall that the averaging GN variant does not take the structure into account. It's the trivial baseline. Compare with single directed graph, both directions, and fully connected. Table.

Comparison with humans, final, best accuracy in a table.

Training runs
\begin{enumerate}
    \item \#3 [time-intense]: pre-train vs. fine-tune vs. end-to-end
    \item \#5 [fast]: GN with averaging, GN with max pooling, GN with 1 core block, GN with 3 core blocks (w and w/o weight sharing)
    \item \#4 [fast]: best of (2.) with different b values
    \item \#3 [fast]: best of (2.) with different edge choices, namely fully-connected, bi-directional, default
\end{enumerate}
