\subsection{Rank Predictor}

We have dubbed our working project \textit{Rank Predictor}. It contains things such as implementation of machine learning models, dataset loader and pipeline, analysis scripts, and training orchestration scripts. Opposed to the GN code it is not a library but rather a loose collection of scripts and classes. In combination with the dataset it may be used to reproduce our results.

In this section we describe the individual components on a high level, enriched with noteworthy details.

\begin{itemize}
    \item \textbf{Analysis}: Scripts and Jupyter notebooks for post-training analysis of model weights, activations, etc.
    \item \textbf{Data}: The subfolders \texttt{data/v1} and \texttt{data/v2} contain code related to both dataset versions, respectively. For each dataset we define a class (\texttt{DatasetV1} and \texttt{DatasetV2}) which extends PyTorch's dataset class \texttt{torch.utils.data.Dataset}. The datasets only hold a list of all files in RAM and load the samples (images and the graph JSON file) on the fly, once a sample is requested. The dataset classes may be reused by researchers who want to work with our dataset as well.\\
    We split our datasets with the method \texttt{get\_threefold}. It deterministically splits the dataset on program-startup. The source code is written down in Listing \ref{lst:getthreefold}.\\
    For dataset version 2 there are two subclasses namely \texttt{DatasetV2Screenshots} and \texttt{DatasetV2Cached}. The former discards all graph information and yields lists of screenshots instead. The latter serves graphs where the nodes are attributed with cached feature vectors, previously computed by a feature extractor model.
    \item \textbf{ML models}: Our ML models are in the \texttt{model} subfolder. Each model is a class inheriting from \texttt{torch.nn.Module} with its own file. For graph networks there are commonly implementations of aggregation and update functions which we place in the model file as well.
    \item \textbf{Trainer}: Orchestration of training runs. We use TensorBoard to monitor training runs and Sacred to log experiments. Our \texttt{TraininRun} class is generic and calls an abstract \texttt{\_train\_step} method for each training step. After several steps it calls \texttt{\_run\_valid} with both train and test dataset to see how well the model performs on those dataset in evaluation mode (dropout disabled). We implement this class for graph network training, feature extractor pre-training, and training on dataset version 1. The trainer folder also contains the probabilistic ranking loss (defined in Section~\ref{sec:loss}) and accuracy metric function (defined in Section~\ref{sec:accuracy}). We have unit tested the critical functions.
\end{itemize}
