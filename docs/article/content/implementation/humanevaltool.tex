\subsection{Human Evaluation Tool}

In order to compare the performance of our pagerank estimation model with humans, we have developed a basic evaluation tool. It allowed us to determine the accuracy of human individuals on the pagerank task.

Figure \ref{fig:humanevalscreenshot} is a screenshot of the tool. The user opens the tool in their webbrowser and sees screenshots from two webpages. Then, the user can examine both of them and must decide which one they suspect to be ranked higher. After the decision, which can be expressed either by clicking onto the page or using the hotkeys, the tool shows the next tuple. A tuple may also be skipped, for instance if one of the pages is all-black. The pages are randomly chosen. The user can constantly see their accuracy and the number of samples they have assessed so far. Reloading the page will not result in loss, because the scores are saved in the browser's local storage. If desired, however, the score can be reset manually.

\begin{figure}\centering
    \makebox[\textwidth]{
        \includegraphics[width=\textwidth]{resources/human-eval-screenshot-min.png}
        }
    \caption{Screenshot of the human evaluation tool}\label{fig:humanevalscreenshot}
\end{figure}

The tool has a Node.js backend which provides an API. It serves the dataset images, meta information such as website ranks, and random tuples. The UI is very simple and has only few CSS styling rules.
