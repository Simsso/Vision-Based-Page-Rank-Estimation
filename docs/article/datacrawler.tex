\section{Datacrawler}
\label{Datacrawler}
The datacrawler represents a very important part and has a critical role in the success of our research. The main purpose of the datacrawler is to collect data and enrich existing data. Specifically, the datacrawler is used to deliver the required dataset versions (described in Section \label{dataset} \improve{[IMPROVE] Add reference to dataset}), used to do research, develop, and train our model.

The input to the datacrawler is simply a domain, which is initially provided by the chosen ground truth in the dataset specification. The output varies between dataset versions. For example, dataset version 1 (specified in Section \label{datasetversion1}) \improve{[IMPROVE] Add reference to dataset version 1} requires a screenshot per given domain.

The following sections will describe the initial conceptual requirements for the datacrawler (\ref{datacrawler_requirements}), the decisions taken towards chosen the frameworks and programming language (\ref{datacrawler_framework_language}) and the current architecture of the datacrawler (\ref{datacrawler_architecture}). Within that the custom plugin-system with so called \textit{DataModules} are introduced (\ref{datacrawler_datamodulesystem}), all developed and consumed DataModules (\ref{datacrawler_screenshot_datamodule}, \ref{datacrawler_mobilescreenshot_datamodule} \improve{[IMPROVE] Add missing refs to new datamodules}) and their interplay as a whole unit is described (\ref{datacrawler_workflow}). \info{[INFO] You might add a section with "Calculating the Graph" or "Outputing the dataset"} In the last section we will discuss how we used state-of-the-art technologies and designed an architecture to scale the datacrawler to dramatically reduce the analysis time (\ref{datacrawler_scale}).

\subsection{Requirements}
\label{datacrawler_requirements}
In general all dataset version specifications (\ref{datasetversion1}, \ref{datasetversion2}) can be seen as requirements for the datacrawler. While a specification generally describes how the given dataset version should look, it also implicitly defines required functionality for the datacrawler. A specifications such as the demand for a screenshot of the given website, implicitly requires the datacrawler to be able to render and take a screenshot of the website.

One of the major requirements for the datacrawler was to find a convenient way of emulating a web browser. This requirement raised from the fact that the main goal of our research was to evaluate a given website and calculate a rank depending on the user experience of the user during their visit of the website. This requirement is furthermore underlined by the fact that any other approaches to calculate attributes of a domain, such as the download size or loading time, would ultimately lead to distortion of the data. Another fact is that for attributes such as screenshot the given website has to be interpreted and rendered on the screen with the highest possible compatibility. The browser was primarily designed for this purpose.

The accessibility to \textit{low-level} information of a website represents another important requirement and is raised from the emulation of a browser. This means that the datacrawler had to have the possibility to access information such as the \textit{HTTP-Request} (\ref{datacrawler_mobilescreenshot_datamodule}) or \textit{Document Object Model} (\ref{datacrawler_url_datamodule}) to be able to calculate specific attributes. So being able to read and manipulate internal data structures or even to inject own Javascript (\ref{datacrawler_screenshot_datamodule}, \ref{datacrawler_mobilescreenshot_datamodule})  represents another inevitable requirement for the datacrawler.

Another requirement was the flexibility regarding to be calculated attributes. In this case flexibility means that each attribute should be calculated independently from each other generating high flexibility in choosing which attributes should be calculated. This flexibility raised from the iterative process of dataset specification: It might happen that dataset version $i$ requires some certain attributes whereas dataset version $i+1$ does not. This flexibility was required and led to a sophisticated architecture such as the \textit{DataModule-System} (\ref{datacrawler_datamodulesystem}), which makes it possible to directly feed the dataset specification into the datacrawler.

Lastly scalability and efficiency in terms of time represented two major requirements for the datacrawler. Considering the huge amount of domains to be analysed, the datacrawler had to be horizontally scalable to allow the analysis of multiple domains at once, and efficient in terms analysis time and start-up time. An inefficient and not scalable datacrawler would lead to multiple days of required analysis time and high infrastructure costs.

\subsection{Chromium Embedded Framework}
\label{datacrawler_framework_language}

\subsection{Datacrawler Architecture}
\label{datacrawler_architecture}

\subsubsection{DataModule-System}
\label{datacrawler_datamodulesystem}

\subsubsection{Screenshot-Datamodule}
\label{datacrawler_screenshot_datamodule}

\subsubsection{MobileScreenshot-Datamodule}
\label{datacrawler_mobilescreenshot_datamodule}

\subsubsection{URL-Datamodule}
\label{datacrawler_url_datamodule}

\improve{[IMPROVE] Add documentation for new datamodules}

\subsubsection{Datacrawler Workflow}
\label{datacrawler_workflow}

\subsection{Scaling the Datacrawler}
\label{datacrawler_scale}

\subsubsection{Requirements}
\label{datacrawler_scale_requirements}

\subsubsection{The Scale Architecture}
\label{datacrawler_scale_architecture}


